{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "3"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "<h1 align=\"center\"> Predicting Credit Card Approvals using ML Techniques </h1>\n",
    "<br>\n",
    "<p>In this notebook, a customized version of a DataCamp project will be presented, whereas an automatic credit card approval predictor using machine learning techniques were built.</p><br>\n",
    "\n",
    "Topics: `Data Manipulation`   `Machine Learning`   `Importing & Cleaning Data`   `Applied Finance`\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "## Credit card applications\n",
    "<p> Owing the amount of credit card applications received by commercial banks and the lenght of time a manual credit analysis would consume, machine learning techniques can be performed in order to automate the predictions of credit card approval. For this purpose, a machine learning model will be built using the <a href=\"http://archive.ics.uci.edu/ml/datasets/credit+approval\">Credit Card Approval dataset</a> from the UCI Machine Learning Repository.</p>\n",
    "<p><img src=\"https://assets.datacamp.com/production/project_558/img/credit_card.jpg\" align=\"center\" width=\"500\" height=\"600\" alt=\"Credit card being held in hand\"></p>\n",
    "<p>The structure of this notebook is organized as follows:</p>\n",
    "<ul>\n",
    "<li>Loading and inspection of the dataset.</li>\n",
    "<li>Data preprocessing prior machine learning model predictions.</li>\n",
    "<li>Exploratory data analysis.</li>\n",
    "<li>Building a machine learning model to predict credit card approval of individual application.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset\n",
    "\n",
    "<p>First, importing <code>pandas</code> package for loading and viewing the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "dc": {
     "key": "3"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  0      1      2  3  4  5  6     7  8  9   10 11 12     13   14 15\n",
       "0  b  30.83  0.000  u  g  w  v  1.25  t  t   1  f  g  00202    0  +\n",
       "1  a  58.67  4.460  u  g  q  h  3.04  t  t   6  f  g  00043  560  +\n",
       "2  a  24.50  0.500  u  g  q  h  1.50  t  f   0  f  g  00280  824  +\n",
       "3  b  27.83  1.540  u  g  w  v  3.75  t  t   5  t  g  00100    3  +\n",
       "4  b  20.17  5.625  u  g  w  v  1.71  t  f   0  f  s  00120    0  +"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ignoring FutureWarning before importing pandas for better aesthetics\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Importing the required package\n",
    "import pandas as pd\n",
    "\n",
    "# Loading the dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None)\n",
    "\n",
    "# Inspecting the first 5 rows of the dataset\n",
    "cc_apps.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "10"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 2. Inspecting the applications\n",
    "\n",
    "<p>The features of this dataset have been anonymized for confidentiality and so the columns are not stated. According to <a href=\"http://rstudio-pubs-static.s3.amazonaws.com/73039_9946de135c0a49daa7a0a9eda4a67a72.html\">this blog</a>, the tipical features in a credit card application are <b>Gender</b>, <b>Age</b>, <b>Debt</b>, <b>Married</b>, <b>BankCustomer</b>, <b>EducationLevel</b>, <b>Ethnicity</b>, <b>YearsEmployed</b>, <b>PriorDefault</b>, <b>Employed</b>, <b>CreditScore</b>, <b>DriversLicense</b>, <b>Citizen</b>, <b>ZipCode</b>, <b>Income</b>, besides <b>ApprovalStatus</b>.</p>\n",
    "\n",
    "<p>To get more information about the DataFrame, the <code>describe()</code>, <code>info()</code> and <code>tail()</code> methods were applied:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "dc": {
     "key": "10"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               2           7          10             14\n",
      "count  690.000000  690.000000  690.00000     690.000000\n",
      "mean     4.758725    2.223406    2.40000    1017.385507\n",
      "std      4.978163    3.346513    4.86294    5210.102598\n",
      "min      0.000000    0.000000    0.00000       0.000000\n",
      "25%      1.000000    0.165000    0.00000       0.000000\n",
      "50%      2.750000    1.000000    0.00000       5.000000\n",
      "75%      7.207500    2.625000    3.00000     395.500000\n",
      "max     28.000000   28.500000   67.00000  100000.000000\n",
      "\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 690 entries, 0 to 689\n",
      "Data columns (total 16 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       690 non-null    object \n",
      " 1   1       690 non-null    object \n",
      " 2   2       690 non-null    float64\n",
      " 3   3       690 non-null    object \n",
      " 4   4       690 non-null    object \n",
      " 5   5       690 non-null    object \n",
      " 6   6       690 non-null    object \n",
      " 7   7       690 non-null    float64\n",
      " 8   8       690 non-null    object \n",
      " 9   9       690 non-null    object \n",
      " 10  10      690 non-null    int64  \n",
      " 11  11      690 non-null    object \n",
      " 12  12      690 non-null    object \n",
      " 13  13      690 non-null    object \n",
      " 14  14      690 non-null    int64  \n",
      " 15  15      690 non-null    object \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 86.4+ KB\n",
      "None\n",
      "\n",
      "\n",
      "    0      1       2  3  4   5   6      7  8  9   10 11 12     13   14 15\n",
      "673  ?  29.50   2.000  y  p   e   h  2.000  f  f   0  f  g  00256   17  -\n",
      "674  a  37.33   2.500  u  g   i   h  0.210  f  f   0  f  g  00260  246  -\n",
      "675  a  41.58   1.040  u  g  aa   v  0.665  f  f   0  f  g  00240  237  -\n",
      "676  a  30.58  10.665  u  g   q   h  0.085  f  t  12  t  g  00129    3  -\n",
      "677  b  19.42   7.250  u  g   m   v  0.040  f  t   1  f  g  00100    1  -\n",
      "678  a  17.92  10.210  u  g  ff  ff  0.000  f  f   0  f  g  00000   50  -\n",
      "679  a  20.08   1.250  u  g   c   v  0.000  f  f   0  f  g  00000    0  -\n",
      "680  b  19.50   0.290  u  g   k   v  0.290  f  f   0  f  g  00280  364  -\n",
      "681  b  27.83   1.000  y  p   d   h  3.000  f  f   0  f  g  00176  537  -\n",
      "682  b  17.08   3.290  u  g   i   v  0.335  f  f   0  t  g  00140    2  -\n",
      "683  b  36.42   0.750  y  p   d   v  0.585  f  f   0  f  g  00240    3  -\n",
      "684  b  40.58   3.290  u  g   m   v  3.500  f  f   0  t  s  00400    0  -\n",
      "685  b  21.08  10.085  y  p   e   h  1.250  f  f   0  f  g  00260    0  -\n",
      "686  a  22.67   0.750  u  g   c   v  2.000  f  t   2  t  g  00200  394  -\n",
      "687  a  25.25  13.500  y  p  ff  ff  2.000  f  t   1  t  g  00200    1  -\n",
      "688  b  17.92   0.205  u  g  aa   v  0.040  f  f   0  f  g  00280  750  -\n",
      "689  b  35.00   3.375  u  g   c   h  8.290  f  f   0  t  g  00000    0  -\n"
     ]
    }
   ],
   "source": [
    "# Printing summary statistics\n",
    "cc_apps_description = cc_apps.describe()\n",
    "print(cc_apps_description)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Printing DataFrame information\n",
    "cc_apps_info = cc_apps.info()\n",
    "print(cc_apps_info)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Inspecting missing values in the dataset\n",
    "print(cc_apps.tail(17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Note that the dataset is composed of missing values, besides both numerical and non-numerical features, making the use of preprocessing necessary.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "17"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 3. Splitting the dataset into train and test sets\n",
    "<p>Bevor preprocessing the data, as preferred, the dataset has been split into train and test set for the training and testing phases of machine learning modeling.</p>\n",
    "\n",
    "<p>Also, a feature selection has been made and unnecessary features such as <b>DriversLicense</b> and <b>ZipCode</b> were dropped.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "dc": {
     "key": "17"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dropping the features DriversLicense and ZipCode\n",
    "cc_apps = cc_apps.drop(columns=[11,13])\n",
    "\n",
    "# Split into train and test sets\n",
    "cc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "24"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 4. Handling the missing values (part I)\n",
    "<p>As identified from inspecting the DataFrame, the dataset contains both numeric and non-numeric data of types <code>float64</code>, <code>int64</code> and <code>object</code> types), whereas the features 2, 7, 10 and 14 contain numeric values, while all the other features contain non-numeric values.</p>\n",
    "\n",
    "<p>Missing values in the dataset are labeled as question marks. As a temporarily solution to help performing further missing value treatment, the question marks were replaced by NaN. For this, the <code>numpy</code> library was used.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "dc": {
     "key": "24"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing numpy\n",
    "import numpy as np\n",
    "\n",
    "# Replacing the '?'s with NaN in the both sets\n",
    "cc_apps_train = cc_apps_train.replace('?', np.nan)\n",
    "cc_apps_test = cc_apps_test.replace('?', np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "31"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 5. Handling the missing values (part II)\n",
    "<p>Since many models such as Linear Discriminant Analysis (LDA) cannot handle missing values implicitly, a strategy called <i>mean imputation</i> was implemented. This is a better way to deal with missing values instead of ignoring them, and consequently affecting the performance of a machine learning model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "dc": {
     "key": "31"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     8\n",
      "1     5\n",
      "2     0\n",
      "3     6\n",
      "4     6\n",
      "5     7\n",
      "6     7\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "12    0\n",
      "14    0\n",
      "15    0\n",
      "dtype: int64\n",
      "0     4\n",
      "1     7\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     2\n",
      "6     2\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "12    0\n",
      "14    0\n",
      "15    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imputing the missing values of the numeric columns with mean imputation\n",
    "cc_apps_train.loc[:,[2,7,10,14]].fillna(cc_apps_train.mean(), inplace=True)\n",
    "cc_apps_test.loc[:,[2,7,10,14]].fillna(cc_apps_train.mean(), inplace=True)\n",
    "\n",
    "# Counting and printing the number of NaNs in the datasets to verify\n",
    "print(cc_apps_train.isnull().sum())\n",
    "print(cc_apps_test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "38"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 6. Handling the missing values (part III)\n",
    "<p>For the columns containing non-numeric data-types, the impute of the missing values were implemented with the most frequent values. This is <a href=\"https://www.datacamp.com/community/tutorials/categorical-data\">good practice</a> when it comes to imputing missing values for categorical data in general.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "dc": {
     "key": "38"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0\n",
      "1     0\n",
      "2     0\n",
      "3     0\n",
      "4     0\n",
      "5     0\n",
      "6     0\n",
      "7     0\n",
      "8     0\n",
      "9     0\n",
      "10    0\n",
      "12    0\n",
      "14    0\n",
      "15    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each column of cc_apps_train\n",
    "for col in list(cc_apps_train):\n",
    "    # Check if the column is of object type\n",
    "    if cc_apps_train[col].dtypes == 'object':\n",
    "        # Impute with the most frequent value\n",
    "        cc_apps_train = cc_apps_train.fillna(cc_apps_train[col].value_counts().index[0])\n",
    "        cc_apps_test = cc_apps_test.fillna(cc_apps_train[col].value_counts().index[0])\n",
    "\n",
    "# Counting and printing the number of NaNs in the dataset to verify\n",
    "print(cc_apps.isna().sum()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "45"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 7. Preprocessing the data (part I)\n",
    "<p>Before proceeding towards building the machine learning model, there are still minor but essential data preprocessing necessary to be done, as described as follow:</p>\n",
    "<ol>\n",
    "<li>Convert the non-numeric data into numeric.</li>\n",
    "<li>Scale the feature values to a uniform range.</li>\n",
    "</ol>\n",
    "<p>Many machine learning models (especially the ones developed using scikit-learn), require the data to be in a strictly numeric format. This can be perfomed using the <code>get_dummies()</code> method from pandas.</p>\n",
    "<p>Another alternative would be the technique called <i>label encoding.</i></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "dc": {
     "key": "45"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Converting the categorical features in the train and test sets independently\n",
    "cc_apps_train = pd.get_dummies(cc_apps_train)\n",
    "cc_apps_test = pd.get_dummies(cc_apps_test)\n",
    "\n",
    "# Reindexing the columns of the test set aligning with the train set\n",
    "cc_apps_test = cc_apps_test.reindex(columns=cc_apps_train.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "52"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 8. Preprocessing the data (part II)\n",
    "<p>When a dataset has varying ranges as observed in this dataset, a small change in a particular feature may not have a significant impact on the other feature. The final preprocessing step is then to scale before fitting the model to the data. Using the <b>CreditScore</b> as an example, the higher this number, the more financially trustworthy a person is considered to be. Therefore, rescaling all the values to a range of 0-1, a <b>CreditScore</b> of 1 is the highest.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "dc": {
     "key": "52"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing MinMaxScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Segregating features and labels into separate variables\n",
    "X_train, y_train = cc_apps_train.iloc[:, :-1].values, cc_apps_train.iloc[:, [-1]].values\n",
    "X_test, y_test = cc_apps_test.iloc[:, :-1].values, cc_apps_test.iloc[:, [-1]].values\n",
    "\n",
    "# Instantiating MinMaxScaler and using it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "59"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 9. Fitting a logistic regression model to the train set\n",
    "<p>Fundamentally, the predicting of credit card application approval is a <a href=\"https://en.wikipedia.org/wiki/Statistical_classification\">classification</a> task. According to UCI, the dataset contains out of 690 instances 383 (55.5%) applications that were denied and 307 (44.5%) applications that were approved. A good machine learning model should be able to accurately predict the status of the applications with respect to these statistics.</p>\n",
    "<p>Generally, linear models perform well in the case where features are correlated with each other. Therefore, initially a machine learning modeling with a Logistic Regression model (a generalized linear model) were applied.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "dc": {
     "key": "59"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Instantiating a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fitting logreg to the train set\n",
    "logreg.fit(rescaledX_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "66"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 10. Making predictions and evaluating performance\n",
    "<p> In the case of predicting credit card applications, it is important to see if this machine learning model is equally capable of predicting approved and denied status, in line with the frequency of these labels in the original dataset. If the model is not performing well in this aspect, then it might end up approving the application that should have been approved. The confusion matrix helps to view the model's performance from these aspects. </p>\n",
    "<p>For the <a href=\"https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/\">confusion matrix</a>, the first element of the first row denotes the true negatives meaning the number of negative instances (denied applications) predicted by the model correctly. And the last element of the second row of the confusion matrix denotes the true positives meaning the number of positive instances (approved applications) predicted by the model correctly.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "dc": {
     "key": "66"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression classifier:  1.0\n",
      "[[103   0]\n",
      " [  0 125]]\n"
     ]
    }
   ],
   "source": [
    "# Import confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Get the accuracy score of logreg model and print it\n",
    "print(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test, y_test))\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The model seems to be pretty good! In fact it was able to yield an accuracy score of 100%</b>\n",
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "73"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 11. Grid searching and making the model perform better\n",
    "<p>If the model hadn't yielded a perfect score, performing a <a href=\"https://machinelearningmastery.com/how-to-tune-algorithm-parameters-with-scikit-learn/\">grid search</a> of the model parameters to improve the model's ability to predict credit card approvals could be a way to perform better. <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\">Scikit-learn's implementation of logistic regression</a> consists of different hyperparameters, such as <code>tol</code> and <code>max_iter</code></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "dc": {
     "key": "73"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [],
   "source": [
    "# Importing GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Defining the grid of values for tol and max_iter\n",
    "tol = [0.01, 0.001, 0.0001]\n",
    "max_iter = [100, 150, 200]\n",
    "\n",
    "# Creating a dictionary where tol and max_iter are keys and the lists of their values are corresponding values\n",
    "param_grid = dict(tol = tol, max_iter = max_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "dc": {
     "key": "80"
    },
    "run_control": {
     "frozen": true
    },
    "tags": [
     "context"
    ]
   },
   "source": [
    "## 12. Finding the best performing model\n",
    "<p>The grid of hyperparameter values has been definied and converted into a single dictionary format which <code>GridSearchCV()</code> expects as one of its parameters. To find out which values perfom best, the grid search can be started, instantiating <code>GridSearchCV()</code> with the earlier <code>logreg</code> and then performing <a href=\"https://www.dataschool.io/machine-learning-with-scikit-learn/\">cross-validation</a> of five folds.</p>\n",
    "\n",
    "<p>Finally, this notebook will end by storing the best-achieved score and the respective best parameters.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "dc": {
     "key": "80"
    },
    "tags": [
     "sample_code"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 1.000000 using {'max_iter': 100, 'tol': 0.01}\n",
      "Accuracy of logistic regression classifier:  1.0\n"
     ]
    }
   ],
   "source": [
    "# Instantiate GridSearchCV with the required parameters\n",
    "grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit grid_model to the data\n",
    "grid_model_result = grid_model.fit(rescaledX_train, y_train.ravel())\n",
    "\n",
    "# Summarize results\n",
    "best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))\n",
    "\n",
    "# Extract the best model and evaluate it on the test set\n",
    "best_model = grid_model_result.best_estimator_\n",
    "print(\"Accuracy of logistic regression classifier: \", best_model.score(rescaledX_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
